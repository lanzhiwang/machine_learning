--------------------------------------------------------------------------------------------------
1ã€Introduction of this course
01_introduction.pptx

å‚è€ƒä¹¦

â€œMachine Learningâ€ (ML) å’Œ â€œMachine Learning and having it Deep and Structuredâ€ (MLDS) æœ‰ä½•ä¸åŒï¼Ÿ

Speech Recognition è¯­éŸ³è¯†åˆ«
Image Recognition å›¾åƒè¯†åˆ«
Playing Go ä¸‹å›´æ£‹
Dialogue System å¯¹è¯ç³»ç»Ÿ

Goodness of function f å‡½æ•°ä¼˜åº¦ f

Learning Theory å­¦ä¹ ç†è®º
scenario è®¾æƒ³
Supervised Learning ç›‘ç£å­¦ä¹ 
Semi-supervised Learning åŠç›‘ç£å­¦ä¹ 
Transfer Learning è¿ç§»å­¦ä¹ 
Unsupervised Learning æ— ç›‘ç£å­¦ä¹ 
Reinforcement Learning å¼ºåŒ–å­¦ä¹ 
task
Regression å›å½’
Classification åˆ†ç±»
Structured Learning ç»“æ„åŒ–å­¦ä¹ 
method
Linear Model çº¿æ€§æ¨¡å‹
Non-linear Model
Deep Learning æ·±åº¦å­¦ä¹ 
SVM, decision tree, K-NN â€¦ SVMã€å†³ç­–æ ‘ã€K-NNâ€¦â€¦

Regression
The output of the target function ğ‘“ is â€œscalarâ€.

Binary Classification äºŒå…ƒåˆ†ç±»
Multi-class Classification å¤šç±»åˆ†ç±»

Spam filtering åƒåœ¾é‚®ä»¶è¿‡æ»¤

Document Classification æ–‡æ¡£åˆ†ç±»

Hierarchical Structure å±‚æ¬¡ç»“æ„

éœ€è¦æ³¨æ„ä¸‹å›´æ£‹çš„è®­ç»ƒæ•°æ®çš„ç»„ç»‡å½¢å¼

åŠç›‘ç£å­¦ä¹ å’Œè¿ç§»å­¦ä¹ çš„åŒºåˆ«æ˜¯åŒæ ·æ˜¯çŒ«ç‹—åˆ†ç±»çš„ä»»åŠ¡ï¼Œ
åŠç›‘ç£å­¦ä¹ ä½¿ç”¨çš„å…¨éƒ¨æ˜¯çŒ«å’Œç‹—çš„å›¾ç‰‡ï¼Œåªä¸è¿‡æœ‰ä¸€éƒ¨åˆ†å›¾ç‰‡æ²¡æœ‰æ ‡ç­¾ï¼Œ
è¿ç§»å­¦ä¹ é™¤äº†ä½¿ç”¨çŒ«å’Œç‹—çš„å›¾ç‰‡ï¼Œè¿˜ä½¿ç”¨äº†å…¶ä»–å›¾ç‰‡ï¼Œè¿™äº›å›¾ç‰‡å’ŒçŒ«ç‹—æ— å…³

Data not related to the task considered (can be either labeled or unlabeled) ä¸è€ƒè™‘çš„ä»»åŠ¡æ— å…³çš„æ•°æ®ï¼ˆå¯ä»¥æ ‡è®°æˆ–ä¸æ ‡è®°ï¼‰

Machine Reading: Machine learns the meaning of words from reading a lot of documents without supervision æœºå™¨é˜…è¯»ï¼šæœºå™¨æ— éœ€ç›‘ç£ï¼Œé€šè¿‡é˜…è¯»å¤§é‡æ–‡æ¡£æ¥å­¦ä¹ å•è¯çš„å«ä¹‰

Beyond Classification è¶…è¶Šåˆ†ç±»
Speech Recognition è¯­éŸ³è¯†åˆ«â€”æ²¡æœ‰åŠæ³•ç©·ä¸¾å£°éŸ³å¯èƒ½å¾—æ‰€æœ‰è¾“å‡º
Machine Translation æœºå™¨ç¿»è¯‘â€”æ²¡æœ‰åŠæ³•ç©·ä¸¾æ‰€æœ‰å¯èƒ½ç¿»è¯‘çš„ç»“æ„

Learning from critics å‘æ‰¹è¯„å®¶å­¦ä¹ 

Convolutional Neural Network (CNN) å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰


--------------------------------------------------------------------------------------------------
2ã€Regression
02_Regression.pptx

Regression å›å½’
Case Study æ¡ˆä¾‹ç ”ç©¶

Stock Market Forecast è‚¡å¸‚é¢„æµ‹
Dow Jones Industrial Average at tomorrow æ˜å¤©çš„é“ç¼æ–¯å·¥ä¸šå¹³å‡æŒ‡æ•°
Self-driving Car
Recommendation

Estimating the Combat Power (CP) of a pokemon after evolution è¯„ä¼°å®å¯æ¢¦è¿›åŒ–åçš„æˆ˜æ–—åŠ› (CP)

feature ç‰¹å¾
weight æƒé‡
bias åå·®
çº¿æ€§æ¨¡å‹å’Œå¤šä¸ªç‰¹å¾å€¼ä¹‹å‰çš„å…³ç³»ï¼šğ‘¦=ğ‘+ âˆ‘â–’ã€–ğ‘¤_ğ‘– ğ‘¥_ğ‘– ã€—

Goodness of Function å‡½æ•°ä¼˜åº¦
Training Data: 10 pokemons è®­ç»ƒæ•°æ®ï¼š10 åª Pokemon

Estimated y based on input function æ ¹æ®è¾“å…¥å‡½æ•°ä¼°è®¡ y
Estimation error ä¼°è®¡è¯¯å·®

The color represents L(ğ‘¤,ğ‘). é¢œè‰²ä»£è¡¨ L(ğ‘¤,ğ‘)ã€‚

Gradient Descent æ¢¯åº¦ä¸‹é™

Negative è´Ÿ
Positive æ­£
Increase w å¢åŠ 
Decrease w å‡å°‘

Î· is called learning rate

Local optimal å±€éƒ¨æœ€ä¼˜
not global optimal ä¸æ˜¯å…¨å±€æœ€ä¼˜

Is this statement correct? è¿™ç§è¯´æ³•æ­£ç¡®å—ï¼Ÿ

Stuck at local minima å¡åœ¨å±€éƒ¨æå°å€¼
Stuck at saddle point å¡åœ¨éç‚¹
Very slow at the plateau åœ¨é«˜åŸä¸Šéå¸¸ç¼“æ…¢

Worry æ‹…å¿ƒ
Donâ€™t worry. In linear regression, the loss function L is convex. åˆ«æ‹…å¿ƒã€‚åœ¨çº¿æ€§å›å½’ä¸­ï¼ŒæŸå¤±å‡½æ•° L æ˜¯å‡¸å‡½æ•°ã€‚
Formulation of ğœ•ğ¿âˆ•ğœ•ğ‘¤ and ğœ•ğ¿âˆ•ğœ•ğ‘ ğœ•ğ¿âˆ•ğœ•ğ‘¤ å’Œ ğœ•ğ¿âˆ•ğœ•ğ‘ çš„å…¬å¼

Generalization æ¦‚æ‹¬
What we really care about is the error on new data (testing data) æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„æ˜¯æ–°æ•°æ®ï¼ˆæµ‹è¯•æ•°æ®ï¼‰ä¸Šçš„é”™è¯¯

Better! Could it be even better? æ›´å¥½ï¼è¿˜èƒ½æ›´å¥½å—ï¼Ÿ

Slightly better. ç¨å¾®å¥½ä¸€ç‚¹ã€‚
How about more complex model? æ›´å¤æ‚çš„æ¨¡å‹æ€ä¹ˆæ ·ï¼Ÿ

The results become worse ... ç»“æœå˜å¾—æ›´ç³Ÿ......

A more complex model yields lower error on training data. æ›´å¤æ‚çš„æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šäº§ç”Ÿçš„é”™è¯¯æ›´å°‘ã€‚
If we can truly find the best function å¦‚æœæˆ‘ä»¬çœŸçš„èƒ½æ‰¾åˆ°æœ€å¥½çš„å‡½æ•°

A more complex model does not always lead to better performance on testing data. æ›´å¤æ‚çš„æ¨¡å‹å¹¶ä¸æ€»æ˜¯èƒ½åœ¨æµ‹è¯•æ•°æ®ä¸Šå¸¦æ¥æ›´å¥½çš„è¡¨ç°ã€‚
This is Overfitting. è¿™å°±æ˜¯è¿‡åº¦æ‹Ÿåˆã€‚
Select suitable model é€‰æ‹©åˆé€‚çš„æ¨¡å‹

There is some hidden factors not considered in the previous model â€¦â€¦ å…¶ä¸­å­˜åœ¨ä¸€äº›å…ˆå‰æ¨¡å‹æœªè€ƒè™‘åˆ°çš„éšè—å› ç´ â€¦â€¦

Pidgey æ³¢æ³¢
Eevee ä¼Šå¸ƒ
Weedle å¨å¾·å°”
Caterpie å¡ç‰¹çš®

Regularization æ­£åˆ™åŒ–
The functions with smaller ğ‘¤_ğ‘– are better ğ‘¤_ğ‘– è¶Šå°çš„å‡½æ•°è¶Šå¥½
We believe smoother function is more likely to be correct æˆ‘ä»¬ç›¸ä¿¡å¹³æ»‘çš„å‡½æ•°æ›´å¯èƒ½æ˜¯æ­£ç¡®çš„
smoother å¹³æ»‘
Why smooth functions are preferred? ä¸ºä»€ä¹ˆæ›´å€¾å‘äºå¹³æ»‘å‡½æ•°ï¼Ÿ
If some noises corrupt input xi when testing å¦‚æœæµ‹è¯•æ—¶æŸäº›å™ªå£°ç ´åäº†è¾“å…¥ xi
A smoother function has less influence. æ›´å¹³æ»‘çš„å‡½æ•°å½±å“è¾ƒå°ã€‚

Training error: largerğœ†, considering the training error less è®­ç»ƒè¯¯å·®ï¼šğœ†è¶Šå¤§ï¼Œè€ƒè™‘åˆ°è®­ç»ƒè¯¯å·®è¶Šå°
We prefer smooth function, but donâ€™t be too smooth. æˆ‘ä»¬æ›´å–œæ¬¢å¹³æ»‘çš„å‡½æ•°ï¼Œä½†ä¸è¦å¤ªå¹³æ»‘ã€‚

Pokemon: Original CP and species almost decide the CP after evolution (there are probably other hidden factors) å£è¢‹å¦–æ€ªï¼šåŸå§‹ CP å’Œç‰©ç§å‡ ä¹å†³å®šäº†è¿›åŒ–åçš„ CPï¼ˆå¯èƒ½è¿˜æœ‰å…¶ä»–éšè—å› ç´ ï¼‰
Gradient descent æ¢¯åº¦ä¸‹é™
Following lectures: theory and tips æ¥ä¸‹æ¥çš„è®²åº§ï¼šç†è®ºå’ŒæŠ€å·§
Overfitting and Regularization è¿‡åº¦æ‹Ÿåˆå’Œæ­£åˆ™åŒ–
Following lectures: more theory behind these æ¥ä¸‹æ¥çš„è®²åº§ï¼šè¿™äº›èƒŒåçš„æ›´å¤šç†è®º
We finally get average error = 11.1 on the testing data æˆ‘ä»¬æœ€ç»ˆåœ¨æµ‹è¯•æ•°æ®ä¸Šå¾—åˆ°å¹³å‡è¯¯å·® = 11.1
How about another set of new data? Underestimate? Overestimate? å¦ä¸€ç»„æ–°æ•°æ®æ€ä¹ˆæ ·ï¼Ÿä½ä¼°ï¼Ÿé«˜ä¼°ï¼Ÿ
Following lectures: validation æ¥ä¸‹æ¥çš„è®²åº§ï¼šéªŒè¯


--------------------------------------------------------------------------------------------------
3ã€Where does the error come from?
03_Bias and Variance.pptx

bias åè§ï¼Œåå·®
variance æ–¹å·®

Estimator ä¼°è®¡é‡

Estimate the mean of a variable x ä¼°è®¡å˜é‡xçš„å‡å€¼
assume the mean of x is ğœ‡ å‡è®¾å‡å€¼
assume the variance of x is ğœ^2 å‡è®¾æ–¹å·®
m è¡¨ç¤ºç®—æœ¯å¹³å‡å€¼
ğ¸[ğ‘š] è¡¨ç¤ºæœŸæœ›å€¼
unbiased æ— åè§çš„

Vğ‘ğ‘Ÿ[ğ‘š] è¡¨ç¤º m çš„æ–¹å·®

s è¡¨ç¤ºæ–¹å·®
ğ¸[s] è¡¨ç¤º s çš„æœŸæœ›å€¼

If we can do the experiments several times

Parallel Universes å¹³è¡Œå®‡å®™
In all the universes, we are collecting (catching) 10 PokÃ©mons as training data to find ğ‘“^âˆ—  åœ¨æ‰€æœ‰çš„å®‡å®™ä¸­ï¼Œæˆ‘ä»¬æ”¶é›†ï¼ˆæ•æ‰ï¼‰10ä¸ªpoksamonsä½œä¸ºè®­ç»ƒæ•°æ®æ¥æ‰¾åˆ°ğ‘“^ *

Simpler model is less influenced by the sampled data ç®€å•çš„æ¨¡å‹å—é‡‡æ ·æ•°æ®çš„å½±å“è¾ƒå°
Consider the extreme case f(x) = c è€ƒè™‘æç«¯æƒ…å†µf(x) = c

We donâ€™t really know the F^

Overfittingè¿‡åº¦æ‹Ÿåˆ
Underfitting æ¬ æ‹Ÿåˆ

Diagnosis: è¯Šæ–­
If your model cannot even fit the training examples, then you have large bias å¦‚æœæ‚¨çš„æ¨¡å‹ç”šè‡³æ— æ³•æ‹Ÿåˆè®­ç»ƒæ ·æœ¬ï¼Œé‚£ä¹ˆæ‚¨å­˜åœ¨è¾ƒå¤§çš„åå·®ã€‚
If you can fit the training data, but large error on testing data, then you probably have large variance å¦‚æœæ‚¨èƒ½å¤Ÿæ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®ä¸Šå­˜åœ¨è¾ƒå¤§è¯¯å·®ï¼Œé‚£ä¹ˆæ‚¨å¯èƒ½å…·æœ‰è¾ƒå¤§çš„æ–¹å·®ã€‚
For bias, redesign your model: å¯¹äºåå·®ï¼Œé‡æ–°è®¾è®¡æ‚¨çš„æ¨¡å‹ï¼š
Add more features as input å¢åŠ æ›´å¤šçš„è¾“å…¥ç‰¹å¾
A more complex model ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹

Very effective, but not always practical éå¸¸æœ‰æ•ˆï¼Œä½†å¹¶ä¸æ€»æ˜¯å®ç”¨

Regularization æ­£åˆ™åŒ–

There is usually a trade-off between bias and variance. åå·®å’Œæ–¹å·®ä¹‹é—´é€šå¸¸å­˜åœ¨æƒè¡¡ã€‚
Select a model that balances two kinds of error to minimize total error é€‰æ‹©ä¸€ä¸ªå¹³è¡¡ä¸¤ç§è¯¯å·®çš„æ¨¡å‹ï¼Œä»¥æœ€å°åŒ–æ€»è¯¯å·®
What you should NOT do: ä½ ä¸åº”è¯¥åšçš„äº‹æƒ…ï¼š


--------------------------------------------------------------------------------------------------
4ã€Gradient Descent
04_Gradient Descent.pptx

Gradient Descent æ¢¯åº¦ä¸‹é™æ³•
Stochastic éšæœº
Data normalization æ•°æ®å½’ä¸€åŒ–

Tuning your learning rates è°ƒæ•´å­¦ä¹ ç‡

Learning Rate å¦‚æœåˆšåˆšå¥½ï¼ŒLearning Rate é¡ºç€çº¢è‰²ç®­å¤´èµ°åˆ°æœ€ä½ç‚¹
Learning Rate å¦‚æœå¤ªå°ï¼Œæ­¥ä¼å¤ªå°ï¼Œå¦‚è“è‰²ç®­å¤´æ‰€ç¤ºï¼Œè™½ç„¶ä¹Ÿä¼šèµ°åˆ°æœ€ä½ç‚¹ï¼Œä½†æ˜¯é€Ÿåº¦ä¼šå¾ˆæ…¢
Learning Rate å¦‚æœå¤ªå¤§ï¼Œæ­¥ä¼å¤ªå¤§ï¼Œå¦‚ç»¿è‰²ç®­å¤´æ‰€ç¤ºï¼Œé‚£ä¹ˆå°±èµ°ä¸åˆ°æœ€ä½ç‚¹ï¼Œå®ƒæ°¸è¿œåœ¨æœ€ä½ç‚¹ä¸¤è¾¹éœ‡è¡
Learning Rate å¦‚æœç‰¹åˆ«å¤§ï¼Œæ­¥ä¼ç‰¹åˆ«å¤§ï¼Œå¦‚é»„è‰²ç®­å¤´æ‰€ç¤ºï¼Œé‚£ä¹ˆå‚æ•°å°±ä¼šè¶Šæ¥è¶Šå¤§ï¼Œæ°¸è¿œä¸èƒ½åˆ°è¾¾æœ€ä½ç‚¹

Adaptive Learning Rates è‡ªé€‚åº”å­¦ä¹ ç‡
Popular & Simple Idea: Reduce the learning rate by some factor every few epochs. æµè¡Œå’Œç®€å•çš„æƒ³æ³•ï¼šæ¯éš”å‡ ä¸ªepochå°±å‡å°‘ä¸€äº›å­¦ä¹ ç‡ã€‚
  At the beginning, we are far from the destination, so we use larger learning rate ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬ç¦»ç›®çš„åœ°å¾ˆè¿œï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
  After several epochs, we are close to the destination, so we reduce the learning rate ç»è¿‡å‡ ä¸ªepochä¹‹åï¼Œæˆ‘ä»¬æ¥è¿‘äº†ç›®çš„åœ°ï¼Œæ‰€ä»¥æˆ‘ä»¬é™ä½äº†å­¦ä¹ ç‡
  E.g. 1/t decay: ğœ‚^ğ‘¡=ğœ‚âˆ•âˆš(ğ‘¡+1) 1/tè¡°å˜
Learning rate cannot be one-size-fits-all å­¦ä¹ ç‡ä¸å¯èƒ½æ˜¯æ”¾ä¹‹å››æµ·è€Œçš†å‡†çš„
Giving different parameters different learning rates ç»™å‡ºä¸åŒçš„å‚æ•°ä¸åŒçš„å­¦ä¹ ç‡

Adagrad
Stochastic Gradient descent éšæœº
Vanilla Gradient descent ä¸€èˆ¬çš„ Gradient descent

Contradiction çŸ›ç›¾

Intuitive Reason ç›´è§‚çš„åŸå› 

http://seed.ucsd.edu/mediawiki/images/6/6a/Adagrad.pdf
http://courses.cs.washington.edu/courses/cse547/15sp/slides/adagrad.pdf

Larger 1st order derivative means far from the minima ä¸€é˜¶å¯¼æ•°è¶Šå¤§ï¼Œè¡¨ç¤ºè·ç¦»æœ€å°å€¼è¶Šè¿œ

Some features can be extremely useful and informative to an optimization problem but they may not show up in most of the training instances or data. If, when they do show up, they are weighted equally in terms of learning rate as a feature that has shown up hundreds of times we are practically saying that the influence of such features means nothing in the overall optimization (it's impact per step in the stochastic gradient descent will be so small that it can practically be discounted). To counter this, AdaGrad makes it such that features that are more sparse in the data have a higher learning rate which translates into a larger update for that feature (i.e. in logistic regression that feature's regression coefficient will be increased/decreased more than a coefficient of a feature that is seen very often). æœ‰äº›ç‰¹å¾å¯èƒ½å¯¹ä¼˜åŒ–é—®é¢˜éå¸¸æœ‰ç”¨ï¼Œä½†å®ƒä»¬å¯èƒ½ä¸ä¼šå‡ºç°åœ¨å¤§å¤šæ•°è®­ç»ƒå®ä¾‹æˆ–æ•°æ®ä¸­ã€‚å¦‚æœï¼Œå½“å®ƒä»¬ç¡®å®å‡ºç°æ—¶ï¼Œå®ƒä»¬åœ¨å­¦ä¹ ç‡æ–¹é¢çš„æƒé‡æ˜¯ç›¸ç­‰çš„ï¼Œä½œä¸ºä¸€ä¸ªå‡ºç°äº†æ•°ç™¾æ¬¡çš„ç‰¹å¾ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨è¯´ï¼Œè¿™äº›ç‰¹å¾çš„å½±å“åœ¨æ•´ä½“ä¼˜åŒ–ä¸­æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼ˆå®ƒåœ¨éšæœºæ¢¯åº¦ä¸‹é™ä¸­çš„æ¯ä¸€æ­¥çš„å½±å“å°†éå¸¸å°ï¼Œå‡ ä¹å¯ä»¥è¢«è´´ç°ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒAdaGradä½¿æ•°æ®ä¸­æ›´ç¨€ç–çš„ç‰¹å¾å…·æœ‰æ›´é«˜çš„å­¦ä¹ ç‡ï¼Œä»è€Œè½¬åŒ–ä¸ºè¯¥ç‰¹å¾çš„æ›´å¤§æ›´æ–°ï¼ˆä¾‹å¦‚ï¼Œåœ¨é€»è¾‘å›å½’ä¸­ï¼Œç‰¹å¾çš„å›å½’ç³»æ•°å°†æ¯”ç»å¸¸çœ‹åˆ°çš„ç‰¹å¾çš„ç³»æ•°å¢åŠ /å‡å°‘æ›´å¤šï¼‰ã€‚

Simply put, sparse features can be very useful. I don't have an example of application in neural network training. Different adaptive learning algorithms are useful with different data (it would really depend on what your data is and how much importance you place on sparse features). ç®€å•åœ°è¯´ï¼Œç¨€ç–ç‰¹å¾éå¸¸æœ‰ç”¨ã€‚æˆ‘æ²¡æœ‰ä¸€ä¸ªåº”ç”¨åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„ä¾‹å­ã€‚ä¸åŒçš„è‡ªé€‚åº”å­¦ä¹ ç®—æ³•å¯¹ä¸åŒçš„æ•°æ®æœ‰ç”¨ï¼ˆè¿™å®é™…ä¸Šå–å†³äºä½ çš„æ•°æ®æ˜¯ä»€ä¹ˆä»¥åŠä½ å¯¹ç¨€ç–ç‰¹å¾çš„é‡è§†ç¨‹åº¦ï¼‰ã€‚

Comparison between different parameters ä¸åŒå‚æ•°æ¯”è¾ƒ

Second DerivativeäºŒé˜¶å¯¼æ•°

Stochastic Gradient descent éšæœº

Two approaches update the parameters towards the same direction, but stochastic is faster!

Feature Scaling ç‰¹å¾ç¼©æ”¾
https://standardfrancis.wordpress.com/2015/04/16/batch-normalization/

Theory ç†è®º
Stuck at local minima å¡åœ¨å±€éƒ¨æå°å€¼
Stuck at saddle point å¡åœ¨éç‚¹
Very slow at the plateau åœ¨é«˜åŸä¸Šéå¸¸ç¼“æ…¢


--------------------------------------------------------------------------------------------------
5ã€Classification: Probabilistic Generative Model
05_Classification (v2).pptx

Classification: Probabilistic Generative Model åˆ†ç±»ï¼šæ¦‚ç‡ç”Ÿæˆæ¨¡å‹

Credit Scoring ä¿¡ç”¨è¯„åˆ†
  Input: income, savings, profession, age, past financial history â€¦â€¦ è¾“å…¥ï¼šæ”¶å…¥ï¼Œå‚¨è“„ï¼ŒèŒä¸šï¼Œå¹´é¾„ï¼Œè¿‡å»çš„è´¢åŠ¡å†å²......
  Output: accept or refuse è¾“å‡ºï¼šacceptæˆ–refuse
Medical Diagnosis åŒ»å­¦è¯Šæ–­
  Input: current symptoms, age, gender, past medical history â€¦â€¦  è¾“å…¥ï¼šå½“å‰ç—‡çŠ¶ï¼Œå¹´é¾„ï¼Œæ€§åˆ«ï¼Œæ—¢å¾€ç—…å²......
  Output: which kind of diseases è¾“å‡ºï¼šå“ªä¸€ç±»ç–¾ç—…
Handwritten character recognition æ‰‹å†™å­—ç¬¦è¯†åˆ«
Face recognition äººè„¸è¯†åˆ«
Input: image of a face, output: person è¾“å…¥ï¼šäººè„¸å›¾åƒï¼Œè¾“å‡ºï¼šäºº

Total: sum of all stats that come after this, a general guide to how strong a pokemon is æ€»å€¼ï¼šåœ¨æ­¤ä¹‹åçš„æ‰€æœ‰å±æ€§çš„æ€»å’Œï¼Œå³å…³äºpokemonçš„å¼ºå¤§ç¨‹åº¦çš„ä¸€èˆ¬æŒ‡å—
HP: hit points, or health, defines how much damage a pokemon can withstand before fainting HPï¼šç”Ÿå‘½å€¼æˆ–ç”Ÿå‘½å€¼å†³å®šäº†pokemonåœ¨æ˜å¥å‰èƒ½å¤Ÿæ‰¿å—å¤šå°‘ä¼¤å®³
Attack: the base modifier for normal attacks (eg. Scratch, Punch) æ”»å‡»:æ™®é€šæ”»å‡»çš„åŸºç¡€ä¿®æ­£å€¼ã€‚åˆ’ç—•,æ‰“å­”)
Defense: the base damage resistance against normal attacks é˜²å¾¡:æŠµæŠ—æ™®é€šæ”»å‡»çš„åŸºç¡€ä¼¤å®³
SP Atk: special attack, the base modifier for special attacks (e.g. fire blast, bubble beam) SPæ”»å‡»ï¼šç‰¹æ®Šæ”»å‡»ï¼Œç‰¹æ®Šæ”»å‡»çš„åŸºç¡€ä¿®é¥°ç¬¦ï¼ˆå¦‚ç«ç„°çˆ†ç‚¸ï¼Œæ°”æ³¡æŸï¼‰
SP Def: the base damage resistance against special attacks SPé˜²å¾¡:å¯¹ç‰¹æ®Šæ”»å‡»çš„åŸºç¡€ä¼¤å®³æŠµæŠ—
Speed: determines which pokemon attacks first each round é€Ÿåº¦ï¼šå†³å®šæ¯ä¸ªå›åˆå“ªä¸ªå£è¢‹å¦–æ€ªå…ˆæ”»å‡»

Can we predict the â€œtypeâ€ of pokemon based on the information? æˆ‘ä»¬èƒ½å¦æ ¹æ®è¿™äº›ä¿¡æ¯é¢„æµ‹pokemonçš„â€œç±»å‹â€ï¼Ÿ

Ideal Alternatives ç†æƒ³çš„æ›¿ä»£å“
Perceptron æ„ŸçŸ¥å™¨

Testing: closer to 1 â†’ class 1; closer to -1 â†’ class 2 
æ¥è¿‘ 1 ä¸ºç¬¬ä¸€ç±»ï¼Œæ¥è¿‘ 2 ä¸ºç¬¬äºŒç±»

to decrease error ä¸ºäº†å‡å°‘è¯¯å·®
Penalize to the examples that are â€œtoo correctâ€ â€¦ æƒ©ç½šé‚£äº›â€œå¤ªæ­£ç¡®â€çš„ä¾‹å­â€¦â€¦

Estimating the Probabilities From training data ä»è®­ç»ƒæ•°æ®ä¼°è®¡æ¦‚ç‡

Prior å…ˆå‰çš„ï¼Œäº‹å…ˆçš„

Gaussian Distribution é«˜æ–¯åˆ†å¸ƒ
Ref: https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/gaussian.pdf

Maximum Likelihood æœ€å¤§ä¼¼ç„¶


--------------------------------------------------------------------------------------------------
6ã€Classification: Logistic Regression
06_Logistic Regression (v4).pptx

Good ref:
http://www.win-vector.com/blog/2011/09/the-simpler-derivation-of-logistic-regression/
http://www.cs.columbia.edu/~smaskey/CS6998/slides/statnlp_week6.pdf
http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf

Classification: Logistic Regression åˆ†ç±»ï¼šé€»è¾‘å›å½’

Cross entropy between two Bernoulli distribution ä¸¤ä¸ªä¼¯åŠªåˆ©åˆ†å¸ƒä¹‹é—´çš„äº¤å‰ç†µ

cross entropy ä»£è¡¨ä¸¤ä¸ª Distribution æœ‰å¤šæ¥è¿‘ï¼Œå¦‚æœ cross entropy ç®—å‡ºæ¥ä¸º 0ï¼Œä»£è¡¨ä¸¤ä¸ª Distribution ä¸€æ¨¡ä¸€æ ·

Discriminative åˆ¤åˆ«æ€§
Generative ç”Ÿæˆæ€§

Will we obtain the same set of w and b? æˆ‘ä»¬ä¼šå¾—åˆ°ç›¸åŒçš„ä¸€ç»„ w å’Œ b å—ï¼Ÿ

The same model (function set), but different function may be selected by the same training data. ç›¸åŒçš„æ¨¡å‹ï¼ˆå‡½æ•°é›†ï¼‰ï¼Œä½†æ˜¯ç›¸åŒçš„è®­ç»ƒæ•°æ®å¯èƒ½ä¼šé€‰å–ä¸åŒçš„å‡½æ•°ã€‚


--------------------------------------------------------------------------------------------------
7ã€Introduction of Deep Learning
07_DL.pptx

Deep learning æ·±åº¦å­¦ä¹ 
attracts lots of attention å¸å¼•äº†å¤§é‡å…³æ³¨
I believe you have seen lots of exciting results before. æˆ‘ç›¸ä¿¡æ‚¨ä¹‹å‰å·²ç»çœ‹åˆ°è¿‡å¾ˆå¤šä»¤äººå…´å¥‹çš„ç»“æœã€‚
Deep learning trends at Google. Source: SIGMOD/Jeff Dean è°·æ­Œçš„æ·±åº¦å­¦ä¹ è¶‹åŠ¿ã€‚èµ„æ–™æ¥æºï¼šSIGMOD/Jeff Dean

1958: Perceptron (linear model) 1958 å¹´ï¼šæ„ŸçŸ¥å™¨ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰
1969: Perceptron has limitation 1969 å¹´ï¼šæ„ŸçŸ¥å™¨å­˜åœ¨å±€é™æ€§
1980s: Multi-layer perceptron 1980 å¹´ä»£ï¼šå¤šå±‚æ„ŸçŸ¥å™¨
Do not have significant difference from DNN today ä¸ä»Šå¤©çš„ DNN æ²¡æœ‰æ˜¾è‘—å·®å¼‚
1986: Backpropagation 1986 å¹´ï¼šåå‘ä¼ æ’­
Usually more than 3 hidden layers is not helpful é€šå¸¸è¶…è¿‡ 3 ä¸ªéšè—å±‚æ²¡æœ‰å¸®åŠ©
1989: 1 hidden layer is â€œgood enoughâ€, why deep? 1989 å¹´ï¼š1 ä¸ªéšè—å±‚â€œè¶³å¤Ÿå¥½â€ï¼Œä¸ºä»€ä¹ˆè¦æ·±ï¼Ÿ
2006: RBM initialization (breakthrough) 2006 å¹´ï¼šRBM åˆå§‹åŒ–ï¼ˆçªç ´ï¼‰
2009: GPU
2011: Start to be popular in speech recognition 2011 å¹´ï¼šå¼€å§‹åœ¨è¯­éŸ³è¯†åˆ«ä¸­æµè¡Œ
2012: win ILSVRC image competition 2012 å¹´ï¼šèµ¢å¾— ILSVRC å›¾åƒç«èµ›

Neural Network ç¥ç»ç½‘ç»œ
Different connection leads to different network structures ä¸åŒçš„è¿æ¥æ–¹å¼å¯¼è‡´ä¸åŒçš„ç½‘ç»œç»“æ„
Network parameter ğœƒ: all the weights and biases in the â€œneuronsâ€ ç½‘ç»œå‚æ•°ğœƒï¼šâ€œç¥ç»å…ƒâ€ä¸­çš„æ‰€æœ‰æƒé‡å’Œåå·®

neuron ç¥ç»å…ƒ

Special structure ç‰¹æ®Šç»“æ„

Using parallel computing techniques to speed up matrix operation åˆ©ç”¨å¹¶è¡Œè®¡ç®—æŠ€æœ¯åŠ é€ŸçŸ©é˜µè¿ç®—

Feature extractor replacing feature engineering ç‰¹å¾æå–å™¨å–ä»£ç‰¹å¾å·¥ç¨‹

Each dimension represents the confidence of a digit. æ¯ä¸ªç»´åº¦ä»£è¡¨ä¸€ä¸ªæ•°å­—çš„ç½®ä¿¡åº¦ã€‚

A function set containing the candidates for Handwriting Digit Recognition åŒ…å«æ‰‹å†™æ•°å­—è¯†åˆ«å€™é€‰å‡½æ•°é›†
You need to decide the network structure to let a good function in your function set. æ‚¨éœ€è¦ç¡®å®šç½‘ç»œç»“æ„ï¼Œä»¥ä¾¿åœ¨å‡½æ•°é›†ä¸­å®ç°è‰¯å¥½çš„å‡½æ•°

Trial and Error åå¤è¯•éªŒ
Intuition ç›´è§‰

Evolutionary Artificial Neural Networks è¿›åŒ–äººå·¥ç¥ç»ç½‘ç»œ

Convolutional Neural Network (CNN) å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰

Cross Entropy äº¤å‰ç†µ

Gradient Descent æ¢¯åº¦ä¸‹é™

I hope you are not too disappointed æˆ‘å¸Œæœ›ä½ ä¸ä¼šå¤ªå¤±æœ›

Backpropagation åå‘ä¼ æ’­

Backpropagation: an efficient way to compute ğœ•ğ¿âˆ•ğœ•ğ‘¤ in neural network åå‘ä¼ æ’­ï¼šåœ¨ç¥ç»ç½‘ç»œä¸­è®¡ç®—ğœ•ğ¿âˆ•ğœ•ğ‘¤çš„æœ‰æ•ˆæ–¹æ³•

Concluding Remarks ç»“æŸè¯­

What are the benefits of deep architecture? æ·±åº¦æ¶æ„æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ

Not surprised, more parameters, better performance ä¸æ„å¤–ï¼Œå‚æ•°æ›´å¤šï¼Œæ€§èƒ½æ›´å¥½

Universality Theorem æ™®éæ€§å®šç†

Any continuous function f ä»»ä½•è¿ç»­å‡½æ•° f
Can be realized by a network with one hidden layer éƒ½å¯ä»¥é€šè¿‡å…·æœ‰ä¸€ä¸ªéšè—å±‚çš„ç½‘ç»œå®ç°
(given enough hidden neurons) ï¼ˆç»™å®šè¶³å¤Ÿçš„éšè—ç¥ç»å…ƒï¼‰
Why â€œDeepâ€ neural network not â€œFatâ€ neural network? ä¸ºä»€ä¹ˆæ˜¯â€œæ·±â€ç¥ç»ç½‘ç»œè€Œä¸æ˜¯â€œèƒ–â€ç¥ç»ç½‘ç»œï¼Ÿ


--------------------------------------------------------------------------------------------------
8ã€Backpropagation
08_BP (v2).pptx

Forward pass å‰ä¼ 
Backward pass åä¼ 


--------------------------------------------------------------------------------------------------
9ã€â€œHello worldâ€ of Deep Learning
09_Keras.pptx

å¸¸è§çš„æ¿€æ´»å‡½æ•°ï¼šsoftplus, softsign, relu, tanh, hard_sigmoid, linear
https://keras.io/api/layers/activations/#available-activations

å¸¸è§æŸå¤±å‡½æ•°ï¼šhttps://keras.io/api/losses/

å¸¸è§ä¼˜åŒ–å‚æ•°æ–¹æ³•ï¼šSGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam
https://keras.io/api/optimizers/

Mini-batch

epoch

Backpropagation åå‘ä¼ æ’­

To compute the gradients efficiently, we use backpropagation. ä¸ºäº†æœ‰æ•ˆåœ°è®¡ç®—æ¢¯åº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨åå‘ä¼ æ’­ã€‚

Very flexible éå¸¸çµæ´»
Need some effort to learn éœ€è¦èŠ±äº›åŠŸå¤«å»å­¦ä¹ 
Interface of TensorFlow or Theano TensorFlow æˆ– Theano çš„æ¥å£

x_trainï¼š(Number of training examples, 28, 20)
y_trainï¼š(Number of training examples, 10)

Batch size influences both speed and performance. You have to tune it. æ‰¹é‡å¤§å°ä¼šå½±å“é€Ÿåº¦å’Œæ€§èƒ½ã€‚æ‚¨å¿…é¡»å¯¹å…¶è¿›è¡Œè°ƒæ•´ã€‚

Shuffle the training examples for each epoch æ¯ä¸ªæ—¶æœŸéƒ½å¯¹è®­ç»ƒæ ·æœ¬è¿›è¡Œæ´—ç‰Œã€‚


--------------------------------------------------------------------------------------------------
10ã€Tips for Deep Learning
10_DNN tip.pptx

CNN ä¸­é—ç•™çš„é—®é¢˜ï¼š
1ã€CNN ä¸­æœ‰ max pooling æ¶æ„ï¼Œä½†æ˜¯ max pooling ä¸èƒ½å¾®åˆ†ï¼Œè¿™ä¸ªåœ¨ Gradient descent è¦æ€ä¹ˆå¤„ç†
2ã€L1 çš„ Regularization æ˜¯ä»€ä¹ˆ

Recipe of Deep Learning æ·±åº¦å­¦ä¹ çš„æµç¨‹

k nearest neighbor kæœ€è¿‘é‚»å±…
decision tree å†³ç­–æ ‘
k nearest neighbor å’Œ decision tree è¿™äº›æ–¹æ³•åœ¨ training data ä¸Šçš„æ­£ç¡®ç‡è‚¯å®šæ˜¯ 100%

Do not always blame Overfitting ä¸è¦æ€»æ˜¯è´£æ€ªè¿‡åº¦æ‹Ÿåˆ

Early Stopping
Regularization æ­£åˆ™åŒ–
Dropout
New activation function
Adaptive Learning Rate


New activation function

Deeper usually does not imply better. æ›´æ·±é€šå¸¸å¹¶ä¸æ„å‘³ç€æ›´å¥½ã€‚
ä¸ºä»€ä¹ˆ

accuracy å‡†ç¡®æ€§

Vanilla Gradient descent ä¸€èˆ¬çš„ Gradient descent

Vanishing Gradient Problem æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
Already converge å·²ç»æ”¶æ•›

Intuitive way to compute the derivatives â€¦ è®¡ç®—å¯¼æ•°çš„ç›´è§‚æ–¹æ³•ã€‚

sigmoid function çš„é—®é¢˜
sigmoid function ä¼šé€ æˆæ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜

ReLU
Fast to compute è®¡ç®—é€Ÿåº¦å¿«
Biological reason ç”Ÿç‰©çš„åŸå› 
Infinite sigmoid with different biases æ— ç©·å¤šä¸ª sigmoid å åŠ å½¢æˆ ReLU
Vanishing gradient problem æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

ä¸ºä»€ä¹ˆ ReLU å¯ä»¥è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜
A Thinner linear network æ›´ç»†çš„çº¿æ€§ç½‘ç»œ
Thinner linear network ç›¸å½“äºæ˜¯ linear çš„ï¼Œä¸ä¼šå‡ºç°æ¢¯åº¦é€æ¸å‡å°çš„é—®é¢˜

ä½¿ç”¨ ReLU ä¼šä½¿æ•´ä¸ª network å˜æˆ linearï¼Œä½†æˆ‘ä»¬éœ€è¦çš„æ˜¯ deep networkï¼Œè¿™æ˜¯çŸ›ç›¾çš„å—ï¼Ÿ

ReLU ä¸èƒ½å¾®åˆ†ï¼Œè¿™ä¸ªè¦æ€ä¹ˆå¤„ç†
ReLU ä¸èƒ½å¾®åˆ†åªæ˜¯åœ¨è¾“å…¥ä¸º 0 çš„æ—¶å€™ï¼Œå…¶ä»–åœ°æ–¹éƒ½æ˜¯å¯ä»¥å¾®åˆ†çš„

ReLU - variant
ğ¿ğ‘’ğ‘ğ‘˜ğ‘¦ ğ‘…ğ‘’ğ¿ğ‘ˆ
ğ‘ƒğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘Ÿğ‘–ğ‘ ğ‘…ğ‘’ğ¿ğ‘ˆ

Maxout network
Maxout network å°±æ˜¯ä½¿ç”¨è®­ç»ƒæ•°æ®è‡ªåŠ¨å­¦ä¹  activation function
ReLU is a special cases of Maxout ReLUæ˜¯Maxoutçš„ä¸€ç§ç‰¹æ®Šæƒ…å†µ

Maxout network æœ‰åŠæ³•åšåˆ°å’Œ ReLU ä¸€æ¨¡ä¸€æ ·çš„äº‹æƒ…ï¼Œå½“ç„¶å®ƒä¹Ÿå¯ä»¥åšåˆ°å…¶ä»– activation function ä¸€æ ·çš„äº‹æƒ…

Maxout network è¦æ€ä¹ˆ training


Adaptive Learning Rate

RMSProp Adagrad çš„è¿›é˜¶ç‰ˆ

Hard to find optimal network parameters éš¾ä»¥æ‰¾åˆ°æœ€ä½³çš„ç½‘ç»œå‚æ•°
Stuck at local minima å¡åœ¨å±€éƒ¨æå°å€¼
Stuck at saddle point å¡åœ¨éç‚¹
Very slow at the plateau åœ¨é«˜åŸä¸Šéå¸¸ç¼“æ…¢
local minima å¾ˆéš¾å‡ºç°ï¼Œä»æ¦‚ç‡çš„è§’åº¦åˆ†æï¼Œå‡è®¾ä¸€ä¸ªå‚æ•°çš„ local minima å‡ºç°çš„æ¦‚ç‡æ˜¯ pï¼Œé‚£ä¹ˆ 1000 ä¸ªå‚æ•°åŒæ—¶å‡ºç° local minima çš„æ¦‚ç‡å°±æ˜¯ p çš„ 1000 æ¬¡æ–¹ï¼Œp çš„ 1000 æ¬¡æ–¹æ˜¯ä¸€ä¸ªå¾ˆå°çš„å€¼ï¼Œæ‰€ä»¥ local minima å¾ˆéš¾å‡ºç°
Momentum å¯ä»¥ä¸€å®šç¨‹åº¦ä¸Šå¤„ç† local minima çš„é—®é¢˜

Momentum åŠ¨é‡
How about put this phenomenon in gradient descent? å¦‚ä½•å°†è¿™ç§ç°è±¡æ”¾å…¥æ¢¯åº¦ä¸‹é™ä¸­ï¼Ÿ

Adam RMSProp + Momentum 


--------------------------------------------------------------------------------------------------
11ã€Convolutional Neural Network
11_CNN.pptx

å·ç§¯ç¥ç»ç½‘ç»œ

Max Pooling ä¸èƒ½å¾®åˆ†è¦æ€ä¹ˆè§£å†³

æš‚æ—¶å¿½ç•¥


--------------------------------------------------------------------------------------------------
12ã€Why Deep?
12_Why.pptx

Modularization æ¨¡å—åŒ–

æš‚æ—¶å¿½ç•¥


--------------------------------------------------------------------------------------------------
13ã€Semi-supervised Learning
13_semi.pptx

Semi-supervised Learning åŠç›‘ç£å­¦ä¹ 

Transductive learning ä¼ å¯¼å­¦ä¹ 
Inductive learning å½’çº³å­¦ä¹ 

Usually with some assumptions é€šå¸¸æœ‰ä¸€äº›å‡è®¾

Outline æ¦‚è¦
Semi-supervised Learning for Generative Model ç”Ÿæˆæ¨¡å‹çš„åŠç›‘ç£å­¦ä¹ 
Low-density Separation Assumption ä½å¯†åº¦åˆ†ç¦»å‡è®¾
Smoothness Assumption å¹³æ»‘åº¦å‡è®¾
Better Representation æ›´å¥½çš„è¡¨ç¤º

Semi-supervised Learning for Generative Model ç”Ÿæˆæ¨¡å‹çš„åŠç›‘ç£å­¦ä¹ 

Decision Boundary å†³ç­–è¾¹ç•Œ

The algorithm converges eventually, but the initialization influences the results. ç®—æ³•æœ€ç»ˆæ”¶æ•›ï¼Œä½†åˆå§‹åŒ–ä¼šå½±å“ç»“æœã€‚

Solved iteratively è¿­ä»£æ±‚è§£

Low-density Separation Assumption ä½å¯†åº¦åˆ†ç¦»å‡è®¾

Self-training

How to choose the data set remains open å¦‚ä½•é€‰æ‹©æ•°æ®é›†ä»æœªç¡®å®š
You can also provide a weight to each data. æ‚¨è¿˜å¯ä»¥ä¸ºæ¯ä¸ªæ•°æ®æä¾›æƒé‡ã€‚

Similar to semi-supervised learning for generative model ç±»ä¼¼äºç”Ÿæˆæ¨¡å‹çš„åŠç›‘ç£å­¦ä¹ 

Entropy-based Regularization åŸºäºç†µçš„æ­£åˆ™åŒ–

Smoothness Assumption å¹³æ»‘åº¦å‡è®¾

More precisely: æ›´å‡†ç¡®åœ°è¯´åº”è¯¥æ˜¯:
x is not uniform. Xä¸æ˜¯å‡åŒ€çš„ã€‚
a high density region é«˜å¯†åº¦åŒºåŸŸ

Represented the data points as a graph å°†æ•°æ®ç‚¹è¡¨ç¤ºä¸ºå›¾å½¢
Graph representation is nature sometimes. å›¾å½¢è¡¨ç¤ºæœ‰æ—¶æ˜¯è‡ªç„¶çš„ã€‚
E.g. Hyperlink of webpages, citation of papers ä¾‹å¦‚ï¼šç½‘é¡µçš„è¶…é“¾æ¥ï¼Œè®ºæ–‡çš„å¼•ç”¨
Sometimes you have to construct the graph yourself. æœ‰æ—¶å€™ä½ å¿…é¡»è‡ªå·±æ„é€ è¿™ä¸ªå›¾ã€‚

K Nearest Neighbor Kè¿‘é‚»
e-Neighborhood eé™„è¿‘


--------------------------------------------------------------------------------------------------
14ã€Unsupervised Learning: Principle Component Analysis
14_PCA (v3).pptx
ä½¿ç”¨ 2016 å¹´çš„ pptï¼Œå› ä¸ºæ‰€æœ‰çš„è§†é¢‘éƒ½æ˜¯ä½¿ç”¨çš„ 2016 å¹´çš„ PPT
14_dim reduction (v5).pptx

Unsupervised Learning: Linear Dimension Reduction æ— ç›‘ç£å­¦ä¹ ï¼šçº¿æ€§é™ç»´

Clustering & Dimension Reduction èšç±»å’Œé™ç»´
Generation ç”Ÿæˆ

èšç±»æ–¹æ³•ï¼š
K-means K å‡å€¼
Hierarchical Agglomerative Clustering (HAC) å±‚æ¬¡å‡èšèšç±» (HAC)

èšç±»æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯èšç±»ä¼šå¼ºåˆ¶æŠŠæ¯ä¸ªäº‹ç‰©éƒ½å½’ç»“åˆ°ä¸€ç±»ä¸­ï¼Œè€Œç°å®æƒ…å†µæ˜¯å¤§éƒ¨åˆ†äº‹ç‰©æ—¢å…·æœ‰Aç±»çš„ç‰¹å¾ï¼Œä¹Ÿæœ‰Bç±»çš„ç‰¹å¾ï¼Œå•çº¯å½’ç»“åˆ°ä¸€ç±»ä¸æ˜¯å¾ˆåˆç†ï¼Œæ‰€ä»¥éœ€è¦é™ç»´ï¼Œé€šè¿‡é™ç»´è¡¨ç¤ºäº‹ç‰©åœ¨æ¯ä¸€ç±»çš„ç‰¹å¾

Distributed Representation åˆ†å¸ƒå¼è¡¨ç¤º
Dimension Reduction é™ç»´

Dimension Reductionæ–¹æ³•ï¼š
Feature selection
Principle component analysis (PCA)

Reduce to 1-D è¯´æ˜ z æ˜¯ä¸€ä¸ª scale

PCA â€“ Another Point of View PCAâ€”â€”å¦ä¸€ç§è§‚ç‚¹

Symmetric å¯¹ç§°
positive-semidefinite åŠæ­£å®š
(non-negative eigenvalues) ï¼ˆéè´Ÿç‰¹å¾å€¼ï¼‰

ğ‘¤^1 is the eigenvector of the covariance matrix S ğ‘¤^1 æ˜¯åæ–¹å·®çŸ©é˜µ S çš„ç‰¹å¾å‘é‡
Corresponding to the largest eigenvalue ğœ†_1 å¯¹åº”äºæœ€å¤§ç‰¹å¾å€¼ ğœ†_1

PCA - decorrelation PCA-å»ç›¸å…³

principle components ä¸»æˆåˆ†
ratio æ¯”ç‡

Non-negative matrix factorization (NMF)

Weakness of PCA PCA çš„å¼±ç‚¹

Matrix Factorization çŸ©é˜µåˆ†è§£

Latent semantic analysis (LSA)


--------------------------------------------------------------------------------------------------
15ã€Unsupervised Learning: Neighbor Embedding
15_TSNE.pptx

Unsupervised Learning: Neighbor Embedding æ— ç›‘ç£å­¦ä¹ ï¼šé‚»åŸŸåµŒå…¥ éçº¿æ€§é™ç»´

Manifold Learning æµå½¢å­¦ä¹ 

Locally Linear Embedding (LLE) å±€éƒ¨çº¿æ€§åµŒå…¥ (LLE)
Laplacian Eigenmaps æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾å›¾
T-distributed Stochastic Neighbor Embedding(t-SNE) T åˆ†å¸ƒéšæœºé‚»åŸŸåµŒå…¥ (t-SNE)

æš‚æ—¶å¿½ç•¥


--------------------------------------------------------------------------------------------------
16ã€Unsupervised Learning: Deep Auto-encoder
16_auto.pptx

Unsupervised Learning: Deep Auto-encoder æ— ç›‘ç£å­¦ä¹ ï¼šæ·±åº¦è‡ªåŠ¨ç¼–ç å™¨

Compact representation of the input object è¾“å…¥å¯¹è±¡çš„ç´§å‡‘è¡¨ç¤º
Can reconstruct the original object èƒ½é‡å»ºåŸæ¥çš„ç‰©ä½“å—
Learn together ä¸€èµ·å­¦ä¹ 

ä¸ºä»€ä¹ˆåŒæ—¶éœ€è¦ç¼–ç å™¨å’Œè§£ç å™¨ï¼Ÿ

Recap: PCA å›é¡¾:ä¸»æˆåˆ†åˆ†æ
As close as possible å°½å¯èƒ½æ¥è¿‘
Bottleneck later ç“¶é¢ˆä¹‹å

Of course, the auto-encoder can be deep å½“ç„¶ï¼Œè‡ªåŠ¨ç¼–ç å™¨å¯ä»¥æ˜¯æ·±åº¦çš„
Initialize by RBM layer-by-layer é€šè¿‡RBMé€å±‚åˆå§‹åŒ–
Symmetric is not necessary. å¯¹ç§°ä¸æ˜¯å¿…é¡»çš„ã€‚

De-noising auto-encoder å»å™ªauto-encoder
Contractive auto-encoder æ”¶ç¼©auto-encoder

Auto-encoder åº”ç”¨ï¼š
Auto-encoder â€“ Text Retrieval è‡ªåŠ¨ç¼–ç å™¨-æ–‡æœ¬æ£€ç´¢
Vector Space Model å‘é‡ç©ºé—´æ¨¡å‹
Bag-of-word 
Semantics are not considered. è¯­ä¹‰ä¸è¢«è€ƒè™‘ã€‚

The documents talking about the same thing will have close code. è®¨è®ºåŒä¸€äº‹ç‰©çš„æ–‡æ¡£å°†å…·æœ‰ç›¸è¿‘çš„ä»£ç ã€‚
LSA: project documents to 2 latent topics LSA: 2ä¸ªæ½œåœ¨ä¸»é¢˜çš„é¡¹ç›®æ–‡ä»¶

Auto-encoder â€“ Similar Image Search è‡ªåŠ¨ç¼–ç å™¨-ç±»ä¼¼çš„å›¾åƒæœç´¢
Retrieved using Euclidean distance in pixel intensity space ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»åœ¨åƒç´ å¼ºåº¦ç©ºé—´æ£€ç´¢

Auto-encoder for CNN
Convolution
Deconvolution
Pooling
Unpooling çš„å®ç°æ–¹æ³•
1ã€è®°ä½åŸæ¥çš„ä½ç½®ï¼Œåœ¨åŸæ¥çš„ä½ç½®ä¸Šå¡«å……å€¼ï¼Œå…¶ä»–åœ°æ–¹å¡«å…… 0
2ã€ç›´æ¥åœ¨æ‰€æœ‰çš„åœ°æ–¹éƒ½å¡«å……ç›¸åŒçš„å€¼

Alternative: simply repeat the values å¯é€‰ï¼šç®€å•åœ°é‡å¤è¿™äº›å€¼
Actually, deconvolution is convolution. å®é™…ä¸Šï¼Œåå·ç§¯å°±æ˜¯å·ç§¯ã€‚

Auto-encoder â€“ Pre-training DNN
Greedy Layer-wise Pre-training again è´ªå©ªçš„åˆ†å±‚é¢„è®­ç»ƒ
DNNï¼ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼‰
RNNï¼ˆé€’å½’ç¥ç»ç½‘ç»œï¼‰
CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰

Find-tune by backpropagation é€šè¿‡åå‘ä¼ æ’­æ‰¾åˆ°è°ƒè°


--------------------------------------------------------------------------------------------------
17ã€Unsupervised Learning: Word Embedding
17_word2vec (v2).pptx

Word Embedding æ˜¯ Dimension Reduction é™ç»´çš„ä¸€ä¸ªåº”ç”¨

ä¸ºä»€ä¹ˆéœ€è¦ Word Embedding

æ€ä¹ˆåš Word Embedding

Word Embedding èƒ½ä¸èƒ½ç”¨ Auto-encoderï¼Ÿ
ä¸èƒ½ï¼Œå› ä¸ºè¯æ±‡çš„å«ä¹‰å’Œè¯æ±‡çš„ä¸Šä¸‹æ–‡æœ‰å…³ï¼Œè€Œ Auto-encoder å…¶å®æ˜¯ä¸è€ƒè™‘ä¸Šä¸‹æ–‡çš„ï¼Œå›¾ç‰‡çš„è¯†åˆ«å°±ä¸éœ€è¦è€ƒè™‘ä¸Šä¸‹æ–‡


--------------------------------------------------------------------------------------------------
18ã€Unsupervised Learning: Deep Generative Model
18_GAN (v3).pptx

å¤ªå¤æ‚äº†ï¼Œæš‚æ—¶å¿½ç•¥


--------------------------------------------------------------------------------------------------
19ã€Transfer Learning
19_transfer.pptx

--------------------------------------------------------------------------------------------------
20ã€Recurrent Neural Network
20_RNN.pptx

--------------------------------------------------------------------------------------------------
21ã€Matrix Factorization
21_MF.pptx

Matrix Factorization çŸ©é˜µåˆ†è§£


--------------------------------------------------------------------------------------------------
22ã€Ensemble
22_Ensemble.pptx

--------------------------------------------------------------------------------------------------
23ã€Introduction of Structured Learning
23_Structured Introduction.pptx

--------------------------------------------------------------------------------------------------
24ã€Introduction of Reinforcement Learning
24_RL (v4).pptx

--------------------------------------------------------------------------------------------------

